<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Video Capture Example</title>
<link href="js_example_style.css" rel="stylesheet" type="text/css" />
</head>
<body>
<h2>Video Capture Example</h2>
<p>
    Test Optical. Will keep to track 100 features.
</p>
<div>
<div class="control"><button id="startAndStop" disabled>Start</button></div>
</div>
<p class="err" id="errorMessage"></p>
<p class="info" id="infoMessage"></p>
<div>
    <table cellpadding="0" cellspacing="0" width="0" border="0">
    <tr>
        <td>
            <video id="videoInput" width=320 height=240></video>
        </td>
        <td>
            <canvas id="canvasOutput" width=320 height=240></canvas>
        </td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>
            <div class="caption">videoInput</div>
        </td>
        <td>
            <div class="caption">canvasOutput</div>
        </td>
        <td></td>
        <td></td>
    </tr>
    </table>
</div>

<script src="https://webrtc.github.io/adapter/adapter-5.0.4.js" type="text/javascript"></script>
<script src="utils.js" type="text/javascript"></script>
<script src="optical_flow_tracker.js" type="text/javascript"></script>

<script type="text/javascript">
let utils = new Utils('errorMessage', 'infoMessage');

//utils.loadCode('codeSnippet', 'codeEditor');

let streaming = false;
let videoInput = document.getElementById('videoInput');
let startAndStop = document.getElementById('startAndStop');
let canvasOutput = document.getElementById('canvasOutput');
let canvasContext = canvasOutput.getContext('2d');

startAndStop.addEventListener('click', () => {
    if (!streaming) {
        utils.clearError();
        //check_devices_and_start_camera(utils, 'qvga', onVideoStarted, 'videoInput');
        // qvga for 320*240 , vga for 640*480
        utils.startCamera('qvga', onVideoStarted, videoInput);
    } else {
        utils.stopCamera();
        onVideoStopped();
    }
});

// let is limited to the block in which it is declared
// while variable declared with var has the global scope.
function onVideoStarted() {
    streaming = true;
    startAndStop.innerText = 'Stop';
    videoInput.width = videoInput.videoWidth;
    videoInput.height = videoInput.videoHeight;
    //utils.executeCode('codeEditor');
    let video = document.getElementById('videoInput');

    let pOpticalFlowTracker = new OpticalFlowTracker(video.height, video.width, 2019);

    let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
    let currGray = new cv.Mat(video.height, video.width, cv.CV_8UC1);
    let prevGray = new cv.Mat(video.height, video.width, cv.CV_8UC1);
    let cap = new cv.VideoCapture(video);

    let prevTrackedPts = new cv.Mat();
    let currTrackedPts = new cv.Mat();
    let status = new cv.Mat();
    let err = new cv.Mat();

    // optical flow parameters
    let winSize = new cv.Size(15, 15);
    let maxLevel = 2;
    let criteria = new cv.TermCriteria(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03);

    // initialize the first frame
    cap.read(src);
    cv.cvtColor(src, prevGray, cv.COLOR_RGBA2GRAY);
    // parameters for ShiTomasi corner detection
    let [maxCorners, qualityLevel, minDistance, blockSize] = [80, 0.2, 20, 7];
    let none = new cv.Mat();
    cv.goodFeaturesToTrack(prevGray, prevTrackedPts, maxCorners, qualityLevel, minDistance, none);

    // create some random colors
    //let color = [];
    //for (let i = 0; i < maxCorners; i++) {
    //    color.push(new cv.Scalar(parseInt(Math.random()*255), parseInt(Math.random()*255),
    //                             parseInt(Math.random()*255), 255));
    //}

    // Create a mask image for drawing purposes
    let mask_default = new cv.Scalar(255);
    let mask_masked = new cv.Scalar(0);
    let color_tracked = new cv.Scalar(255,0,0,255);
    let color_new = new cv.Scalar(0,255,0,255);
    let font = cv.FONT_HERSHEY_SIMPLEX;
    let mask = new cv.Mat(video.height, video.width, cv.CV_8UC1, mask_default);

    let FPS = -1;
    let time_accu = 0;
    let count_fps = 0;

    function processVideo() {
        try {
            if (!streaming) {
                // clean and stop.
                src.delete(); currGray.delete(); prevGray.delete();
                prevTrackedPts.delete(); currTrackedPts.delete(); err.delete();mask.delete();
                return;
            }
            let begin = Date.now();
            // start processing.
            cap.read(src);
            cv.cvtColor(src, currGray, cv.COLOR_RGBA2GRAY);

            // calculate optical flow
            cv.calcOpticalFlowPyrLK(prevGray, currGray, prevTrackedPts, currTrackedPts, status, err, winSize, maxLevel, criteria);

            // select good points and make mask
            let goodNew = [];
            let goodOld = [];
            mask = new cv.Mat(video.height, video.width, cv.CV_8UC1, mask_default);
            let currTrackedPts_data = currTrackedPts.data32F;
            let prevTrackedPts_data = prevTrackedPts.data32F;
            for (let i = 0; i < status.rows; i++) {
              if (status.data[i] === 1) {
                let pt = new cv.Point(currTrackedPts_data[i*2], currTrackedPts_data[i*2+1]);
                goodNew.push(pt);
                goodOld.push(new cv.Point(prevTrackedPts_data[i*2], prevTrackedPts_data[i*2+1]));
                cv.circle(mask, pt, minDistance, mask_masked, -1);
                cv.circle(src, pt, 5, color_tracked, -1);
              }
            }

            // detect new feature for tracking
            let newTrackPts = new cv.Mat();
            let total_pts = goodNew.length;
            let n_to_detect = maxCorners - goodNew.length;
            if(n_to_detect > 0) {
              cv.goodFeaturesToTrack(currGray, newTrackPts, n_to_detect, qualityLevel, minDistance, mask);
              let newTrackPts_data = newTrackPts.data32F;
              //console.log("rows::" + newTrackPts.rows + " cols::" + newTrackPts.cols + " type::" + newTrackPts.type());
              for (let i = 0; i < newTrackPts.rows; i++) {
                let pt = new cv.Point(newTrackPts_data[i*2], newTrackPts_data[i*2+1])
                cv.circle(src, pt, 5, color_new, -1);
              }
              total_pts = goodNew.length+newTrackPts.rows;
            }

            cv.putText(src, '#Pts:'+total_pts+"/"+goodNew.length+', FPS:'+FPS,  new cv.Point(10, 20), font, 0.5, new cv.Scalar(0, 255, 255,255), 1, cv.LINE_4);
            cv.imshow('canvasOutput', src);

            // now update the previous frame and previous points
            currGray.copyTo(prevGray);
            prevTrackedPts.delete(); prevTrackedPts = null;
            prevTrackedPts = new cv.Mat(total_pts, 1, cv.CV_32FC2);
            prevTrackedPts_data = prevTrackedPts.data32F;
            for (let i = 0; i < goodNew.length; i++) {
              prevTrackedPts_data[i*2] = goodNew[i].x;
              prevTrackedPts_data[i*2+1] = goodNew[i].y;
            }
            if(n_to_detect > 0){
              for (let i = 0; i < newTrackPts.rows; i++) {
                let tmp = (i+goodNew.length)*2;
                prevTrackedPts_data[tmp] = newTrackPts.data32F[i*2];
                prevTrackedPts_data[tmp+1] = newTrackPts.data32F[i*2+1];
              }
            }

            // schedule the next one.
            //let delay = 1000/FPS - (Date.now() - begin);
            time_accu = time_accu + (Date.now() - begin)/1000;
            count_fps = count_fps + 1;
            FPS = parseInt(count_fps/time_accu);
            setTimeout(processVideo, 0);
        } catch (err) {
            utils.printError(err);
        }
    };

    // schedule the first one.
    setTimeout(processVideo, 0);
}

function onVideoStopped() {
    streaming = false;
    canvasContext.clearRect(0, 0, canvasOutput.width, canvasOutput.height);
    startAndStop.innerText = 'Start';
}

utils.loadOpenCv(() => {
    startAndStop.removeAttribute('disabled');
});
</script>
</body>
</html>
